\documentclass{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}


\parskip 12pt

\begin{document}

\tableofcontents

\chapter{Einleitung}
\section{Problembeschreibung}
Bei synchronen Flow-Shop-Problemen handelt es sich um Produktions\-planungs\-probleme,
bei denen die zu produzierenden Güter (Jobs) z.B. auf einer zyklisch angeordneten Produktionsanlage
produziert werden. Die Produktionsanlage be\-steht aus $m$ Stationen, die sich mit der Anlage drehen.
Außen, um die Anlage herum, befinden sich $m$ fortlaufend nummerierte fixierte Maschinen $M_1,\ldots,M_m$, die die einzelnen Produktionsschritte durchführen.
Dabei handelt es sich bei Maschine $M_1$ um das Einlegen des Jobs in die Anlage und bei Maschine $M_m$ um die Entnahme des fertigen Produkts.
Durch Rotation der Anlage werden die Stationen mit den auf ihnen befindlichen Jobs zur jeweils nächsten Maschine transportiert.
Die Reihenfolge, in der alle Jobs die Maschinen durchlaufen müssen ist also fest vorgegeben.
In Abbildung \ref{abb:Anlage} ist eine solche zyklische Anlage dargestellt.
\begin{figure}
    \begin{center}
        \includegraphics[width=.6\textwidth]{graphics/anlage.pdf}
    \end{center}
    \caption{
        \label{abb:Anlage}
        Kreisförmige Anlage mit $m=8$ Maschinen.
    }
\end{figure}
Eine Rotation darf immer nur dann stattfinden, wenn alle Maschinen ihren Produktionsschritt an ihrem aktuellen Job
durchgeführt haben. Auf diese Weise können die Jobs, im Gegensatz zum klassischen (asynchronen) Flow-Shop, 
immer nur \textit{synchron} zur nachfolgenden Maschine gelangen.
Die Zeit, die zwischen zwei Rotationen vergeht, wird als \textit{Zykluszeit} bezeichnet.

Die zu produzierenden Jobs sind gegeben durch die Menge $J=\{j_1,\ldots,j_n\}$ 
und die Prozesszeiten von Job $j$ auf Maschine $M_i$ sind durch $p_{ij}$ gegeben.
Eine Beispielinstanz mit $n=5$ und $m=3$ ist in Tabelle \ref{abb:Bsp} zu sehen.
\begin{figure}
    \begin{center}
        \begin{tabular}{c|ccc}
            & $M_1$ & $M_2$ & $M_3$ \\ \hline
            $j_1$ & 4 & 6 & 5 \\
            $j_2$ & 1 & 5 & 6 \\
            $j_3$ & 2 & 5 & 4 \\
            $j_4$ & 5 & 2 & 4 \\
            $j_5$ & 4 & 5 & 4
        \end{tabular}
    \end{center}
    \caption{
        \label{abb:Bsp}
        Beispielinstanz mit 5 Jobs und 3 Maschinen. Die Werte in der Tabelle sind die Prozesszeiten $p_{ij}$.
    }
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{graphics/bspinit.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=.8\textwidth]{graphics/bspopt.pdf}
    \end{center}
    \caption{
        \label{abb:gantt}
        Gantt-Diagramme der initialen Reihenfolge von Beispiel \ref{abb:Bsp} mit $C_{\max}=34$
        und einer optimalen Reihenfolge mit $C_{\max}=30$.
    }
\end{figure}
Ziel ist es, eine Permutation $\pi$ der Jobs zu erstellen, die die gesamte Produktionsdauer minimiert.
Diese Zielfunktion wird mit $C_{\max}$ bezeichnet.
Die Beispielinstanz \ref{abb:Bsp} ist in Abbildung \ref{abb:gantt} als Gantt-Diagramm aufgetragen.
Oben sind die Jobs in der initialen Reihenfolge und unten in einer bezüglich $C_{\max}$ optimalen Reihenfolge.
Die Zykluszeiten $c_t$ mit $1\leq t\leq n+m-1$ berechnen sich wie folgt:
\[ c_t = \max_{i=\max\{1,t-n+1\}}^{\min\{t,m\}} p_{i\pi_{t-i+1}} \]
Die Zielfunktion lässt sich also durch $C_{\max} = \sum_{t=1}^{n+m-1} c_t$ berechnen.
Andere Zielfunktionen werden in dieser Masterarbeit nicht betrachtet.

Eine Teilmenge $D \subseteq \{M_1,\ldots,M_m\}$ der Maschinen heißt \textit{dominierend}, wenn 
\[ p_{dj} \geq p_{ej} \quad \forall j\in J, d\in D, e\not\in D \] 
ist. Die Prozesszeiten aller Jobs auf dominierenden Maschinen sind also immer mindestens so groß wie die Prozesszeiten auf den restlichen Maschinen.
Treten dominierende Maschinen auf, müssen für die Berechnung der Zykluszeiten die Prozesszeiten auf den übrigen Maschinen also nicht betrachtet werden.

Zusätzlich benötigen die Jobs Ressourcen aus einer Menge $R$, um in die Stationen eingelegt werden zu können. Diese Ressourcen können erst nach
Fertigstellung eines Jobs, also nachdem er an Maschine $M_m$ aus der Anlage genommen wurde, wiederverwendet werden.
Sie sind allerdings nur in begrenzter Zahl vorhanden und im Allgemeinen ist nicht jede Ressource für jeden Job geeignet.
Für $j\in J$ sei $\rho_j\subseteq R$ die Menge der Ressourcen, die für $j$ geeignet ist.
Umgekehrt sei für $r\in R$ mit $\iota_r\subseteq J$ die Menge der Jobs bezeichnet, für die $r$ geeignet ist.
An Maschine $M_1$ kann es daher notwendig sein, vor dem Einlegen des nächsten Jobs die Ressource zu wechseln, 
wenn auf der entsprechenden Station zuvor Job $j\in J$ mit Ressource $r\in\rho_j$ fertiggestellt wurde 
und nun Job $j'\not\in\iota_r$ eingelegt werden soll.

Für die Ressourcen können folgende Situationen auftreten:
\begin{itemize}
    \item Alle Ressourcen sind für alle Jobs geeignet, also $\rho_j=R$ für alle $j\in J$.
    \item Die Jobs lassen sich in disjunkte Gruppen unterteilen, so dass für alle Jobs aus einer Gruppe dieselbe Ressourcenmenge geeignet ist.
        Wenn also $\rho_i \cap \rho_j \neq \emptyset$, dann folgt $\rho_i=\rho_j$.
    \item Die Ressourcenmengen bilden Hierarchien. 
        D.h., wenn $\iota_q \cap \iota_r \neq \emptyset$, dann folgt $\iota_q \subseteq \iota_r$ oder $\iota_r \subseteq \iota_q$.
    \item Die $\rho_j$ sind beliebige Teilmengen von $R$.
\end{itemize}%
Neben dem Wechsel von Ressourcen, der zusätzliche Zeit in Anspruch nimmt, können auch andere Formen von \textit{Rüstkosten}
auftreten. Z.B. kann es sein, dass an einer Station zunächst einige Umstellungen vorgenommen werden müssen, bevor der
neue Job eingelegt werden kann. Die Jobs können in Familien $\mathcal{F}$ eingeteilt werden, so dass beim Übergang
zwischen zwei Jobs aus den Familien $f$ und $g$ die Rüstkosten $s_{fg}$ auftreten.
Diese Rüstkosten können 
\begin{itemize}
    \item sowohl vom Vorgänger als auch vom Nachfolger abhängig sein ($s_{fg}$), 
    \item nur vom Nachfolger abhängig sein ($s_{fg} = s_{g}$) oder
    \item konstant sein ($s_{fg} = s > 0$).
\end{itemize}%
Dabei wird $s_{ff} = 0$ angenommen für alle $f\in\mathcal{F}$, dass also keine Rüstkosten innerhalb einer Familie auftreten.
Wenn Rüstkosten auftreten, soll nicht mehr nur die Summe aller Zykluszeiten minimiert werden, sondern zusätzlich noch die Summe aller auftretenden Rüstkosten.
Diese Zielfunktion lässt sich als 
\[ \min\left(C_{\max}+\sum_{f,g\in\mathcal{F}}s_{fg}y_{fg}\right) \] 
formulieren, wobei die Variablen $y_{fg}$ angeben, wie oft die Rüstkosten $s_{fg}$ von einem Job aus Familie $f$ zu einem aus Familie $g$ auftreten.

\begin{figure}
    \begin{center}
        \begin{tabular}{c|ccccc}
            & $j_1$ & $j_2$ & $j_3$ & $j_4$ & $j_5$ \\ \hline
            $j_1$ & 0 & 8 & 5 & 9 & 4\\
            $j_2$ & 6 & 0 & 6 & 8 & 8\\
            $j_3$ & 4 & 7 & 0 & 5 & 4\\
            $j_4$ & 5 & 7 & 8 & 0 & 9\\
            $j_5$ & 4 & 5 & 8 & 9 & 0
        \end{tabular}
    \end{center}
    \caption{
        \label{abb:BspRes}
        Rüstkosten für die Beispielinstanz aus Tabelle \ref{abb:Bsp}.
        Beispielsweise treten Rüstkosten von $6$ auf, wenn von $j_2$ auf $j_1$ gewechselt werden muss.
    }
\end{figure}
In Abbildung \ref{abb:BspRes} ist eine mögliche Rüstkostentabelle für das Beispiel aus \ref{abb:Bsp} gegeben.
Die initiale Reihenfolge und eine optimale sind in Abbildung \ref{abb:ganttres} als Gantt-Diagramm dargestellt.
\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/bspresinit.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=\textwidth]{graphics/bspresopt.pdf}
    \end{center}
    \caption{
        \label{abb:ganttres}
        Initiale und eine optimale Lösung als Gantt-Diagramme für das Beispiel aus \ref{abb:Bsp} erweitert um Rüstkosten aus \ref{abb:BspRes}.
    }
\end{figure}

Insgesamt gilt es, neben der Reihenfolge $\pi$ auch ein Mapping $f:J\rightarrow R$ zu finden, 
das jedem Job $j\in J$ eine Ressource $r\in\rho_j$ zuweist und folgenden Ansprüchen genügt:
\begin{itemize}
    \item $f$ muss zulässig sein in dem Sinne, dass beim Einlegen jedes Jobs $j\in J$ eine Ressource $r\in\rho_j$ verfügbar ist
        (d.h., dass $r$ sich nicht gerade an anderer Stelle in der Anlage befindet).
    \item $f$ und $\pi$ zusammen sollen optimal sein in dem Sinne, dass die Summe aus den durch $\pi$ und $f$ definierten Zykluszeiten und 
        Rüstkosten minimal ist.
\end{itemize}

\section{Motivation}
In \cite{...} wurde gezeigt, dass schon das Optimieren bezüglich $C_{\max}$, also ohne Ressourcen und Rüstkosten, $\mathcal{NP}$-schwer ist.
Versuche, dieses Problem -- oder auch einige Spezialfälle davon -- mit ganzzahliger linearer Programmierung optimal zu lösen, waren nur für sehr kleine Instanzen mit
$n<30$ in hinnehmbarer Zeit erfolgreich, was weit hinter praktischen Anforderungen zurückliegt (vgl. Abschnitt \ref{subsec:LineareProgrammierung}. 
Aufgrund der Komplexität des Gesamtproblems sollen in dieser Arbeit zwei Dekompositionsansätze verfolgt werden:
\begin{enumerate}
    \item Zunächst wird eine Reihenfolge $\pi$ aufgestellt, ohne Ressourcen und Rüstkosten zu betrachten, so dass $C_{\max}$ möglichst gut ist.
        Anschließend wird ein Mapping $f$ basierend auf $\pi$ erstellt, möglichst ohne nachträgliche Änderung an $\pi$ vorzunehmen.
    \item Es wird zuerst ein Mapping $f$ erstellt, so dass die Ressourcen zulässig zugewiesen sind und die durch die Ressourcenwechsel verursachten Rüstkosten minimal sind.
        Anschließend werden, ohne $f$ zu verändern, die Jobs so in einer Reihenfolge $\pi$ angeordnet, dass die Zykluszeiten möglichst minimal sind.
\end{enumerate}%
Beide Ansätze liefern natürlich im Allgemeinen keine optimalen Lösungen, da jeweils getrennt bezüglich $\pi$ und $f$ optimiert wird,
obwohl die optimale Lösung von beiden zusammen abhängig ist.
Außerdem sind selbst die aus den Ansätzen resultierenden Teilprobleme teilweise noch $\mathcal{NP}$-schwer.
Beispielsweise ist bei Ansatz (1) das Berechnen einer $C_{\max}$-optimalen Reihenfolge $\pi$ in dem Spezialfall,
dass es nur eine dominierende Maschine gibt trivial, da $C_{\max}$ bei jeder Reihenfolge identisch ist,
und wenn es genau zwei benachbarte dominierende Maschinen gibt,
ist es mit dem Algorithmus von Gilmore und Gomory \cite{...} in Polynomialzeit lösbar.
Bei der anschließenden Zuweisung von Ressourcen ist noch unbekannt, ob ein polynomieller Algorithmus existiert.
%Dies herauszufinden ist eines der Ziele dieser Arbeit.

Je nachdem, ob die Rüstkosten die Zykluszeiten dominieren oder umgekehrt, ist Ansatz (2) bzw. Ansatz (1) vielversprechender.

\section{Bisherige Ansätze}
\label{sec:bisherigeAnsaetze}
Als weitere Motivation für die Dekompositionsansätze dient folgendes Mixed Integer Linear Program (MIP):
\begin{align}
    \text{min} \quad \sum_{t=1}^{n+m-1} &c_t + \sum_{j=1}^n \sum_{h=1}^n s_{jh} y_{jh} \label{allmip:obj}\\
    \text{s.t.}\quad \sum_{k=1}^n x_{jk} &= 1 & j\in N \label{allmip:1}\\
                     \sum_{j=1}^n x_{jk} &= 1 & k\in N \label{allmip:2}\\
    c_t &\geq \sum_{j=1}^n p_{t-k+1,j} \cdot x_{jk} & t\in T, k=\max\{1,t-m+1\},\ldots,\min\{n,t\} \label{allmip:3}\\
    y_{jh} + 1 &\geq x_{j,k-m} + x_{hk} & j,h\in N, k=m+1,\ldots,n \label{allmip:4}\\
    c_t &\geq 0 & t\in T \\
    x_{jk} &\in \{0,1\} & j,k\in N \\
    y_{jh} &\in \{0,1\} & j,h\in N
\end{align}
$N$ ist die Indexmenge der Jobs, also $N=\{1,\ldots,n\}$ und $T=\{1,\ldots,n+m-1\}$ ist die Indexmenge der Zykluszeiten.
Die Binärvariablen $x_{jk}$ geben an, an welcher Position $k$
sich Job $j$ befindet. Es gilt $x_{jk}=1$ genau dann, wenn $j$ an
Position $k$ ist. Die Nebenbedingungen \ref{allmip:1} und \ref{allmip:2}
stellen sicher, dass sich jeder Job an genau einer Position befindet und 
dass sich an jeder Position genau ein Job befindet.
In Nebenbedingung \ref{allmip:3} werden die Zykluszeiten bestimmt.
Den Binärvariablen $y_{jh}$ wird in Nebenbedingung \ref{allmip:4} folgende
Bedeutung gegeben: $y_{jh}=1$, wenn Job $j$ genau $m$ Positionen
vor Job $h$ in $\pi$ liegt.
In der Zielfunktion \ref{allmip:obj} wird dann die Summe aus allen
Zykluszeiten gebildet und die Summe aller Rüstkosten $s_{jh}$, die beim
Übergang von Job $j$ zu $h$ auftreten.

Obwohl in diesem MIP nicht die Verfügbarkeiten von Ressourcen beachtet werden,
benötigt es schon bei $n=20$ mehrere Stunden zum Finden der optimalen Lösung.
Es ist also für Instanzen mit mehreren Tausend Jobs nicht geeignet.
Aufgrund dieser Tatsache ist eine heuristische Herangehensweise an 
synchrone Flow-Shop-Probleme mit Ressourcen und Rüstkosten eine gute Alternative.


\chapter{Der erste Dekompositionsansatz}
Beim ersten Dekompositionsansatz wird zunächst eine (möglichst gute) Jobreihenfolge $\pi$ bestimmt.
Dabei werden Ressourcen und Rüstkosten außer Acht gelassen. 
Es wird also zunächst ausschließlich $C_{\max}$ optimiert. Anschließend wird ein Mapping $f$ aufgestellt,
so dass Ressourcen nur dann eingeplant werden, wenn sie auch zur Verfügung stehen, und darüber hinaus
möglichst selten ausgetauscht werden müssen, so dass geringe Rüstkosten auftreten.

In Abschnitt \ref{sec:BerechnenEinerJobreihenfolge} werden einige exakte und heuristische Verfahren für die
Berechnung einer $C_{\max}$-optimalen Reihenfolge $\pi$ vorgestellt, was im Allgemeinen $\mathcal{NP}$-schwer ist.
Anschließend werden in Abschnitt \ref{sec:ZuweisungVonRessourcen} Verfahren vorgestellt, die ein möglichst gutes Mapping $f$ erzeugen.
Dabei wird speziell darauf eingegangen, ob mit der gegebenen Reihenfolge $\pi$ und den gegebenen Ressourcen
überhaupt eine zulässige Lösung möglich ist, und, wie dann ggf. die Zulässigkeit durch nachträgliche Änderungen an $\pi$
erzeugt werden kann.

\section{Berechnen einer Jobreihenfolge}
\label{sec:BerechnenEinerJobreihenfolge}
\subsection{Gilmore Gomory}
Der Algorithmus von Gilmore und Gomory \cite{...} löst in $\mathcal{O}(n\log n)$ einen speziellen Fall des gerichteten Travelling-Salesman-Problems (TSP),
bei dem alle Knoten zwei Parameter $x,y$ haben und die Kantenkosten $c_{ij}$ zwischen je zwei Knoten $i$ und $j$ nur vom $x$-Wert von $i$
und vom $y$-Wert von $j$ abhängig sind.

Diese Situation liegt beim synchronen Flow-Shop vor, wenn es nur zwei benachbarte dominierende Maschinen gibt.
O.B.d.A seien dies $M_1$ und $M_2$.
Jobs können durch Knoten repräsentiert werden und die beiden Prozesszeiten auf den dominierenden Maschinen liefern die Parameter $x$ und $y$. 
Der Abstand zwischen zwei Knoten entspricht dann der Zykluszeit, die die entsprechenden Jobs verursachen, wenn sie nebeneinander liegen.
Die Berechnung der Zykluszeiten vereinfacht sich hier zu
$c_t = \max \{p_{2\pi_{t-1}},p_{1\pi_t}\}$ für $2\leq t\leq n$.
Sie sind also für je zwei Jobs $\pi_{t-1},\pi_{t}$ nur noch von der Prozesszeit des vorderen Jobs auf der zweiten Maschine ($p_{2\pi_{t-1}}$) 
und der Prozesszeit des hinteren Jobs auf der ersten Maschine ($p_{1\pi_t}$) abhängig.

Auf die Funktionsweise des Algorithmus soll in dieser Arbeit nicht näher eingegangen werden.
Eine Anwendung des Algorithmus von Gilmore und Gomory als Heuristik für ein Problem mit mehreren dominierenden Maschinen wird in
\ref{subsubsec:ggHeuristik} beschrieben.


\subsection{Ganzzahlige Lineare Programmierung}
\label{subsec:LineareProgrammierung}
Zum Finden einer $C_{\max}$-optimalen Lösung dieses Teilproblems wurde folgendes MIP aufgestellt.
Es ist identisch mit dem MIP in Abschnitt \ref{sec:bisherigeAnsaetze} bis auf die Nichtberücksichtigung der Rüstkosten.
\begin{align}
    \text{min} \quad \sum_{t=1}^{n+m-1} &c_t \\
    \text{s.t.}\quad \sum_{k=1}^n x_{jk} &= 1 & j\in N \\
                     \sum_{j=1}^n x_{jk} &= 1 & k\in N \\
    c_t &\geq \sum_{j=1}^n p_{t-k+1,j} \cdot x_{jk} & t\in T, k=\max\{1,t-m+1\},\ldots,\min\{n,t\} \\
    c_t &\geq 0 & t\in T \\
    x_{jk} &\in \{0,1\} & j,k\in N
\end{align}
Dieses MIP liefert für Instanzen mit $n\leq 30$ eine optimale Lösung in unter einer Stunde.
Diese Laufzeiten sind zwar schon deutlich besser als die Variante mit Rüstkosten in Abschnitt \ref{sec:bisherigeAnsaetze},
bei größeren Instanzen ist dieser Zeitaufwand allerdings immer noch nicht praktikabel. 

Für Instanzen mit $n\leq 500$ ist die beste nach einer halben Stunde gefundene Lösung in ihrer Güte vergleichbar mit den Heuristiken,
die in Unterabschnitt \ref{subsec:HeuristischeVerfahren} vorgestellt werden (für einen Vergleich s. \ref{subsec:VergleichDerHeuristiken}).
In diesem Sinne kann dieses MIP daher ebenfalls als Heuristik betrachtet werden.


\subsection{Heuristische Verfahren}
\label{subsec:HeuristischeVerfahren}
Aufgrund der $\mathcal{NP}$-Schwere der Optimierung von $C_{\max}$ im allgemeinen Fall und der schlechten Laufzeit des MIPs aus Abschnitt \ref{subsec:LineareProgrammierung}
bei Instanzen realer Größe werden hier einige heuristische Ansätze zur Berechnung von $\pi$ vorgestellt.

\subsubsection{Non-Full-Schedule-Heuristik}
Diese Heuristik ist eine konstruktive Greedy-Heuristik, die Schritt für Schritt einen Job an $\pi$ anhängt, beginnend mit einer leeren Reihenfolge.
Sie arbeitet ähnlich wie die Nearest-Neighbor-Heuristik beim TSP.
In jeder Iteration werden alle noch verbleibenden Jobs bewertet und der Job mit der besten Bewertung wird an $\pi$ angehängt.
Die Heuristik benötigt also genau $n$ Iterationen.
Die Bewertungsfunktion betrachtet die letzten $m-1$ Zykluszeiten der noch nicht fertigen Reihenfolge, 
wobei die Zykluszeiten am Anfang bei einer noch leeren Reihenfolge als $0$ angenommen werden.
Für jeden Job $j$, der noch nicht in $\pi$ ist, werden diese $m-1$ Zykluszeiten mit den ersten $m-1$ Prozesszeiten von $j$ verglichen.
Die Idee ist, dass diese möglichst übereinstimmen sollten. Ist eine Prozesszeit sehr viel größer als die aktuelle Zykluszeit,
zu der sie hinzugefügt werden würde, würde die Zykluszeit entsprechend um einen Wert $c_+$ ansteigen.
Ist umgekehrt die Zykluszeit sehr viel größer als die zugehörige Prozesszeit von $j$, dann würde diese kurze Prozesszeit verschenkt werden.
Die Differenz aus Zykluszeit und Prozesszeit wird mit $c_-$ bezeichnet.
Die Bewertungsfunktion berechnet für jeden Job die Summe aus den $m-1$ $c_+$-Werten.
Diese Summe ist die Bewertung für einen Job $j$. Der Job mit der kleinsten Bewertung wird an $\pi$ angehängt.
Falls mehrere Jobs eine optimale Bewertung haben, wird für diese Jobs als zweites Kriterium die Summe der $c_-$-Werte betrachtet.
Der Job, bei dem diese Summe am kleinsten ist, verschenkt am wenigsten Zeit und wird ausgewählt.
Das Vorgehen dieser Heuristik wird anhand der Beispielinstanz \ref{abb:Bsp} in Abbildung \ref{abb:nfsbsp} veranschaulicht.
\begin{figure}
    \begin{center}
%        \includegraphics[width=.8\textwidth]{nfsbsp.pdf}
    \end{center}
    \caption{
        \label{abb:nfsbsp}
    }
\end{figure}

Da in jeder Iteration alle verbleibenden Jobs betrachtet werden und für jeden dieser Jobs $m-1$ Zeiten verglichen werden,
liegt die asymptotische Laufzeit dieser Heuristik in $\mathcal{O}(n^2m)$.

\subsubsection{Double Ended Non-Full-Schedule-Heuristik}
Diese Heuristik basiert auf der Non-Full-Schedule-Heuristik.
Der Unterschied besteht darin, dass $\pi$ nicht nur von vorne, sondern gleichzeitig auch von hinten zur Mitte hin aufgebaut wird.
Jeder noch nicht in $\pi$ enthaltende Job wird pro Iteration mit beiden Enden der bisherigen Reihenfolge verglichen.
Die Bewertungsfunktion für das hintere Ende arbeitet analog.
Es wird der beste Job für das vordere Ende und der beste für das hintere Ende gesucht und der mit der besseren Bewertung wird vorne bzw. hinten eingefügt.
Die asymptotische Laufzeit liegt hier ebenfalls in $\mathcal{O}(n^2m)$.

\subsubsection{Gilmore-Gomory-Heuristik}
\label{subsubsec:ggHeuristik}
Diese Heuristik wendet den Algorithmus von Gilmore und Gomory auf beliebige Instanzen an.
Es können zwei Fälle eintreten:
\begin{enumerate}
    \item Unter den dominierenden Maschinen $D$ gibt es zwei, die benachbart sind, also $\exists i\in\{1,\ldots,m\}, M,M'\in D$ mit $M=M_i$ und $M'=M_{i+1}$.
    \item Es gibt keine benachbarten dominierenden Maschinen.
\end{enumerate}
Wenn Fall (2) eintritt, werden alle $m$ Maschinen als dominierend angesehen, da dies für den Algorithmus keine Einschränkung darstellt.
Nun werden zwei benachbarte dominierende Maschinen gewählt.
O.B.d.A seien dies die Maschinen $M_1$ und $M_2$. Seien 
\begin{align} 
    d_{\min} &\coloneqq \min_{\substack{i\in \{1,2\} \\ j\in J}} p_{ij} \\
    e_{\max} &\coloneqq \max_{\substack{i\not\in \{1,2\} \\ j\in J}} p_{ij} \\
    K &\coloneqq \frac{e_{\max}}{d_{\min}} \text{,}
\end{align}
wobei $d_{\min}=0$ und daraus folgend $K=\infty$ erlaubt sind. 
Nun wird aus der gegebenen Instanz $I$ eine neue Instanz $I'$ erzeugt, die sich nur dadurch von $I$ unterscheidet, dass die Prozesszeiten
auf allen dominierenden Maschinen außer auf $M_1$ und $M_2$ mit $\frac{1}{K}$ skaliert werden, also
\begin{align}
    p'_{ij} \coloneqq \begin{cases} \frac{p_{ij}}{K} &\text{für } i\in D\setminus\{M_1,M_2\} \\ p_{ij} &\text{sonst.} \end{cases}
\end{align}
Auf diese Weise sind $M_1$ und $M_2$ in $I'$ die einzigen dominierenden Maschinen und folglich kann $I'$ mit dem Algorithmus von Gilmore und Gomory optimal gelöst werden.
Die resultierende Reihenfolge $\pi$ wird als heuristische Lösung für die ursprüngliche Instanz $I$ verwendet.

Da die Prozesszeiten der in $I'$ vernachlässigten dominierenden Maschinen sich um den Faktor $K$ von denen in $I$ unterscheiden, 
können sich die aus $\pi$ ergebenen Zykluszeiten von $I$ und $I'$ auch maximal um den Faktor $K$ unterscheiden:
\begin{align}
    c_t \leq K\cdot c'_t \quad \forall 1\leq t\leq n+m-1
\end{align}
Da die optimale Lösung von $I'$ eine untere Schranke für die optimale Lösung von $I$ ist, also $C'_{\max} \leq C_{\max}$, folgt für die Lösung $L_{GG}$ der
Gilmore-Gomory-Heuristik:
\begin{align}
    L_{GG} \leq K\cdot C'_{\max} \leq K\cdot C_{\max}
\end{align}
Die Gilmore-Gomory-Heuristik hat also eine relative Gütegarantie von $K$.
Zusätzlich kann bei der Wahl der zwei benachbarten dominierenden Maschinen der Parameter $K$ optimiert werden, indem Maschinen nicht zufällig gewählt werden,
sondern so, dass die $d_{\min}$- bzw. $e_{\max}$-Werte optimal sind (und somit auch $K$).

Das finden der geeigneten dominierenden Maschinen kann in $\mathcal{O}(nm)$ durchgeführt werden. 
Das Anpassen der Prozesszeiten der übrigen dominierenden Maschinen ist nur in der Theorie von Interesse.
Der Gilmore-Gomory-Algorithmus kann diese einfach ignorieren.
Die Gesamtlaufzeit beträgt daher $\mathcal{O}(nm + n\log n)$.

Für die weitere Analyse dieser Heuristik soll der Begriff der \textit{Semidominanz} eingeführt werden.
Die Semidominanz ist ein Maß dafür, wie weit eine Teilmenge von Maschinen davon entfernt ist, dominierend zu sein.
Eine Semidominanz von $0$ bedeutet, die Maschinen sind dominierend.
Für eine Teilmenge von Maschinen $D\subseteq\{M_1,\ldots,M_m\}$ sei 
\[d_{\min} = \min_{\substack{i:M_i\in D \\ j\in\{1,\ldots,n\}}} p_{ij}\] 
die kleinste ihrer Prozesszeiten, analog zur obigen Definition von $d_{\min}$.
Die Semidominanz ist dann definiert als
\[\mathcal{D}_{D} \coloneqq \sum_{i: M_i\not\in D} \sum_{j=1}^n \max\{0,p_{ij}-d_{\min}\} \text{.}\]
Unabhängig von der \textit{relativen} Gütegarantie $K$ liefert diese Heuristik auch eine \textit{absolute} Gütegarantie von $\mathcal{D}_{\{M_i,M_{i+1}\}}$, 
wenn $M_i$ und $M_{i+1}$ als benachbarte Maschinen ausgewählt werden.
Die Gilmore-Gomory-Heuristik liefert also besonders gute Näherungen, wenn zwei benachbarte Maschinen fast dominierend sind und nur wenige Prozesszeiten auf anderen
Maschinen dies verhindern.

\subsubsection{Nachbarschaftssuche}
Mit einer Nachbarschaftssuche können bereits existierende (nicht optimale) Reihenfolgen verbessert werden.
Mögliche Nachbarschaftsoperatoren sind
\begin{itemize}
    \item $\mathit{xch}_{ij}$: vertauscht die Jobs an den Positionen $i$ und $j$ miteinander,
    \item $\mathit{shift}_{ij}$: "`shiftet"' den Job an Position $i$ nach Position $j$. 
        Alle Jobs zwischen $i$ und $j$ rücken um eins nach links (falls $i<j$) bzw. nach rechts (falls $i>j$).
\end{itemize}
Auf Grundlage dieser Nachbarschaften lassen sich außerdem z.B. Iterative Improvement, eine Tabu-Suche oder Simulated Annealing implementieren.

Im Rahmen dieser Arbeit wurde ein Simulated Annealing Algorithmus auf Grundlage der $\mathit{xch}_{ij}$-Nachbarschaft implementiert.
Es wird min einer zufälligen initialen Reihenfolge $\pi$ begonnen.
Pro Iteration werden zufällige $i$ und $j$ bestimmt, mit denen eine benachbarte Lösung $\pi' = \mathit{xch}_{ij}(\pi)$ erzeugt wird.
Ist $\pi'$ besser als die aktuelle Lösung $\pi$, d.h. $c(\pi')\geq c(\pi)$, wird $\pi'$ als neue Lösung übernommen.
Ist $\pi'$ schlechter als $\pi$, wird $\pi'$ nur mit einer bestimmten Wahrscheinlichkeit übernommen.
Diese Wahrscheinlichkeit beträgt $e^{\frac{c(\pi')-c(\pi)}{t_k}}$, wobei $(t_k)_k$ eine Nullfolge ist 
und $k$ die Anzahl der bisher durchgeführten Iterationen angibt.
Die Folge $(t_k)_k$ wird auch als "`Temperatur"' bezeichnet, die im Laufe des Verfahrens abnimmt.
Es wurde mit unterschiedlichen Temperaturen experimentiert.
Es wurden 
\begin{itemize}
    \item $t_k = \alpha t_{k-1}$ für unterschiedliche $\alpha\in]0,1[$ und 
    \item $t_k = \frac{c}{k}$, 
    \item $t_k = \frac{c}{\log k}$ und 
    \item $t_k = c \frac{|\cos\frac{k}{30}|}{k}$
\end{itemize}
jeweils für unterschiedliche $c\in\mathbb{R}_+$ getestet. Die besten Resultate lieferte $t_k = 50\frac{|\cos\frac{k}{30}|}{k}$.
Durch den Cosinus schwankt die Temperatur, so dass abwechselnd verschlechternde Lösungen erlaubt bzw. nicht erlaubt werden.
Das Verfahren terminiert, sobald in 1000 aufeinander folgenden Iterationen keine neue beste Lösung gefunden werden konnte.

Dieser Simulated Annealing Algorithmus lieferte in Tests meist etwas bessere Ergebnisse als die übrigen Heuristiken.
Die Heuristiken werden im nachfolgenden Unterabschnitt \ref{subsec:VergleichDerHeuristiken} evaluiert.


\subsection{Vergleich der Heuristiken}
\label{subsec:VergleichDerHeuristiken}
In diesem Abschnitt werden die Rechenergebnisse der Heuristiken aus Unterabschnitt \ref{subsec:HeuristischeVerfahren} vorgestellt und miteinander verglichen.
Es wurden drei Testreihen mit zufälligen Instanzen erzeugt, jeweils mit $m=8$ und $n=50,100,150,\ldots,5000$.
Getestet wurde auf einem PC mit Intel(R) Xeon(R) CPU E3-1230 v3, 8 Kerne @ 3.30GHz, und 16GB RAM unter Ubuntu 13.10.
Die MIPs wurden mit ZIMPL \cite{Koch2004} und CPLEX v12.6 gelöst.
Die übrigen Heuristiken wurden in C++ implementiert und mit dem GCC v4.8.1 mit -O2-Optimierung compiliert.

In der ersten Testreihe sind die Maschinen $M_3,M_4,M_5$ dominierend.
Ihre Prozesszeiten wurden gleichverteilt aus dem Intervall $[10,100]$ gewählt.
Die Prozesszeiten der übrigen Maschinen wurden aus dem Intervall $[0,9]$ gleichverteilt gewählt.
In Abbildung \ref{abb:3dom} sind die Resultate aufgetragen.
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom/plot.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom/plotrel.pdf}
    \end{center}
    \caption{
        \label{abb:3dom}
        Absolute (oben) und relative (unten) Unterschiede zwischen der Non-Full-Schedule-Heuristik und den übrigen Heuristiken
        bei drei dominierenden Maschinen (gleichverteilte Prozesszeiten).
    }
\end{figure}
Aufgetragen sind jeweils die absoluten Differenzen $C_{\max,h}-C_{\max,nfs}$ der übrigen Heuristiken ($h$) zur Non-Full-Schedule-Heuristik ($nfs$).
CPLEX LB und UB bezeichnen die untere und obere Schranke, die CPLEX nach 30 Minuten errechnet hat.
Dabei ist die obere Schranke der Zielfunktionswert einer tatsächlich errechneten Lösung und kann daher als Heuristik verwendet werden.
Zu sehen ist, dass die Lösungen des MIPs bei sehr kleinen Instanzen ($n<300$) nach einer halben Stunde Rechenzeit vergleichbar mit den Lösungen der übrigen Heuristiken sind.
Allerdings benutzt CPLEX alle 8 Prozessorkerne parallel, während die übrigen Heuristiken nicht parallel implementiert sind.
Außerdem liegen die Rechenzeiten der übrigen Heuristiken weit unter einer halben Stunde (s. Abbildung \ref{abb:3domtime}).
Aus diesen Gründen wurde das MIP auch nur bis zu Instanzgrößen von $n=1500$ berechnet.
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom/plottime.pdf}
    \end{center}
    \caption{
        \label{abb:3domtime}
        Laufzeiten der drei Heuristiken bei drei dominierenden Maschinen (gleichverteilte Prozesszeiten).
    }
\end{figure}

Außerdem ist zu erkennen, dass die Double Ended Non-Full-Schedule-Heuristik fast identische Resultate liefert im Vergleich zur Non-Full-Schedule-Heuristik
und dass die Gilmore-Gomory-Heuristik hier relativ schlechte Resultate liefert und mit Simulated Annealing die besten Lösungen erzeugt werden.
Der Plot der relativen Abweichungen zeigt außerdem, dass diese Abweichungen offenbar nahezu konstant sind zwischen den Heuristiken (nicht beim MIP) 
und nicht von der Instanzgröße abhängen.
Einzig die relative Abweichung des Simulated Annealings scheint mit zunehmender Instanzgröße leicht abzunehmen.

In Abbildung \ref{abb:3domtime} sind die Zeiten aufgetragen, die die Heuristiken benötigten, um die in Abbildung \ref{abb:3dom} dargestellten Resultate zu berechnen.
Deutlich zu erkennen ist der quadratische Anstieg der beiden Non-Full-Schedule-Heuristiken, wobei die Double Ended Variante etwas langsamer ist,
da pro Iteration immer zwei Jobs statt nur einem gesucht werden.
Außerdem treten in der Kurve der Double Ended Non-Full-Schedule-Heuristik teilweise kleine Ausreißer nach oben auf.
Eine plausible Begründung hierfür konnte nicht gefunden werden.
Die Kurve des Simulated Annealings ist, wie zu erwarten war, sehr chaotisch, was leicht durch die nicht deterministische Funktionsweise des Algorithmus erklärt werden kann.
Am schnellsten ist die Gilmore-Gomory-Heuristik, die dafür aber auch vergleichsweise schlechte Resultate liefert.

Die zweite Testreihe unterscheidet sich nur dadurch von der ersten, dass die Prozesszeiten der dominierenden Maschinen nicht gleichverteilt aus dem Intervall
$[10,100]$ ausgewählt wurden, sondern normalverteilt mit einem Erwartungswert von $\mu=50$ und einer Standardabweichung von $\sigma=20$.
Die Resultate sind analog zur ersten Testreihe in Abbildung \ref{abb:3domnorm} dargestellt und die Laufzeiten dazu in Abbildung \ref{abb:3domnormtime}.
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom_norm/plot.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom_norm/plotrel.pdf}
    \end{center}
    \caption{
        \label{abb:3domnorm}
        Absolute (oben) und relative (unten) Unterschiede zwischen der Non-Full-Schedule-Heuristik und den übrigen Heuristiken
        bei drei dominierenden Maschinen (normalverteilte Prozesszeiten).
    }
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/3dom_norm/plottime.pdf}
    \end{center}
    \caption{
        \label{abb:3domnormtime}
        Laufzeiten der drei Heuristiken bei drei dominierenden Maschinen (normalverteilte Prozesszeiten).
    }
\end{figure}

Die Laufzeiten der zweiten Testreihe unterscheiden nur marginal von denen der ersten, was zu erwarten war, 
da die Laufzeiten der Heuristiken nicht von den Prozesszeiten abhängig sind.
Die Kurven in Abbildung \ref{abb:3domnorm} unterscheiden sich im Verlauf kaum von denen in Abbildung \ref{abb:3dom}.
Allerdings sind diese Kurven im Vergleich zur ersten Testreihe in etwa um den Faktor $2$ gestaucht.
Die Resultate der Heuristiken unterscheiden sich hier also weniger.
Das muss daran liegen, dass die Prozesszeiten (da sie normalverteilt sind) weniger unterschiedlich sind.
Die beiden Non-Full-Schedule-Heuristiken ziehen daraus einen Vorteil, da sie nach eben diesem Prinzip vorgehen, möglichst ähnliche Prozesszeiten zu finden.
Auch die Gilmore-Gomory-Heuristik profitiert davon, da beim (theoretischen) Wiederhochskalieren der Prozesszeiten der dritten dominierenden Maschine 
nur vergleichsweise wenige Zykluszeiten mit vergrößert werden müssen.
Und auch CPLEX kommt bei diesen Daten offenbar ebenfalls schneller zu besseren Schranken.

Die dritte und letzte Testreihe wurde mit einem identischen Verfahren wie die zweite Testreihe generiert.
Allerdings wurde der Erwartungswert der Prozesszeiten auf Maschine $M_3$ auf $\mu=30$ festgesetzt, also um $20$ weniger gegenüber $M_4$ und $M_5$.
Ziel sollte es sein, dass $M_4$ und $M_5$ nicht dominierend sind, sondern nur eine geringe Semidominanz haben, 
so dass die Vorzüge der Gilmore-Gomory-Heuristik zum Tragen kommen.
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/2quasidom/plot.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/2quasidom/plotrel.pdf}
    \end{center}
    \caption{
        \label{abb:2quasidom}
        Absolute (oben) und relative (unten) Unterschiede zwischen der Non-Full-Schedule-Heuristik und den übrigen Heuristiken
        bei zwei semidominierenden Maschinen (normalverteilte Prozesszeiten).
    }
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=.8\textwidth]{../instances/2quasidom/plottime.pdf}
    \end{center}
    \caption{
        \label{abb:2quasidomtime}
        Laufzeiten der drei Heuristiken bei zwei semidominierenden Maschinen (normalverteilte Prozesszeiten).
    }
\end{figure}

Und tatsächlich bestätigen die Rechenergebnisse diese Vermutung, wie in Abbildung \ref{abb:2quasidom} zu sehen ist.
Fast alle Heuristiken liefern bessere Ergebnisse als sie es bei den ersten beiden Testreihen tun (in der gleichen Zeit, s. Abbildung \ref{abb:2quasidomtime}).
Simulated Annealing ist hier allerdings nicht mehr die beste Heuristik.
Die besten Resultate liefern hier die Gilmore-Gomory-Heuristik, die die größte Verbesserung gegenüber den anderen Testreihen erfährt, 
und die Non-Full-Schedule-Heuristik. Die Resultate von beiden sind nahezu identisch.
Die Double Ended Non-Full-Schedule-Heuristik ist hier bei allen Instanzen schlechter als die Non-Full-Schedule-Heuristik, wenn auch nur um weniger als $1\%$
bzw. bis zu $3\%$ bei sehr kleinen Instanzen.
Bemerkenswert ist außerdem, dass die untere Schranke, die von CPLEX für Instanzen mit $n\leq 1500$ berechnet wurde,
oft bis an die Ergebnisse der Gilmore-Gomory- und der Non-Full-Schedule-Heuristik heranreicht.
Das bedeutet, dass die Resultate dieser beiden Heuristiken nahezu optimal sind.
Andererseits sind die oberen Schranken von CPLEX vergleichsweise weit von den Resultaten der Heuristiken entfernt.
Als Ausblick könnten diese Heuristiken daher in CPLEX integriert werden, um die oberen Schranken zu verbessern.



\section{Zuweisung von Ressourcen}
\label{sec:ZuweisungVonRessourcen}
In diesem Abschnitt geht es darum, den Jobs in der Reihenfolge $\pi$, die in Abschnitt \ref{sec:BerechnenEinerJobreihenfolge} aufgestellt wurde,
Ressourcen zuzuweisen. An diese Zuweisung werden zwei Anforderungen gestellt:
\begin{enumerate}
    \item Die Zuweisung muss zulässig sein. Das heißt, ist eine Ressource einem Job zugewiesen, darf sie den nachfolgenden $m-1$ Jobs nicht mehr zugewiesen werden.
        Das Problem der Zulässigkeit einer Zuweisung wird im ersten Unterabschnitt \ref{subsec:ZulaessigkeitDerZuweisung} betrachtet.
    \item Die Zuweisung soll möglichst optimal sein. Wenn zwei Jobs, die im Abstand von $m$ in $\pi$ liegen, die selbe Ressource benutzen können,
        werden Rüstkosten eingespart, da die Ressource nicht ausgetauscht werden muss.
        Dieses Problem wird in Unterabschnitt \ref{subsec:OptimierungDerRessourcenzuweisung} diskutiert.
\end{enumerate}
Es ist natürlich möglich, dass bei gegebenem $\pi$ keine zulässige Zuweisung von Ressourcen möglich ist.
In diesem Fall muss $\pi$ nachträglich geändert werden, worauf in Unterabschnitt \ref{UnzulaessigeReihenfolgen} eingegangen wird.

\subsection{Zulässigkeit der Zuweisung}
\label{subsec:ZulaessigkeitDerZuweisung}
Abhängig davon, welche Ressourcen für welche Jobs geeignet sind, kann das Zulässigkeitsproblem unterschiedlich schwer zu lösen sein.
Folgende Situationen werden betrachtet:
\begin{itemize}
    \item Alle Ressourcen sind für alle Jobs geeignet (trivial, es müssen $m$ Ressourcen vorhanden sein).
    \item Die Ressourcenmengen sind disjunkt ($\rho_i\cap\rho_j \neq \emptyset \enspace \Rightarrow \enspace \rho_i = \rho_j$).
    \item Die Ressourcen sind für hierarchische Jobgruppen geeignet 
        ($\iota_q\cap\iota_r \neq \emptyset \enspace \Rightarrow \enspace \iota_q\subseteq\iota_r$ oder $\iota_r\subseteq\iota_q$).
    \item Die $\rho_i$ sind beliebige Teilmengen von $R$.
\end{itemize}

\subsubsection{Zulässigkeit bei disjunkten Ressourcenmengen}
Bei disjunkten Ressourcenmengen können die Jobs in Gruppen unterteilt werden, 
so dass zu jeder Jobgruppe eine für sie exklusive Menge an zulässigen Ressourcen zur Verfügung steht.
Für die Zulässigkeit reicht es aus, statt der Ressourcenmengen $\rho_i$ nur deren Größen $|\rho_i|$ zu betrachten
(bei der Optimierung der Zuteilung ist diese Vereinfachung nicht sinnvoll).
Mit einem Greedy-Algorithmus kann eine gegebene Reihenfolge $\pi$ dann auf Zulässigkeit überprüft werden:
Der Algorithmus durchläuft $\pi$ von vorne nach hinten.
Jede Jobgruppe erhält einen Counter, der mitzählt, wie viele Ressourcen momentan für sie zur Verfügung stehen.
Diese Counter werden zu Beginn mit $|\rho_i|$ initialisiert.
An jeder Position in $\pi$ wird der Counter der zugehörigen Jobgruppe dekrementiert.
Nach $m$ Schritten wird dieser Counter wieder inkrementiert.
Sollte ein Counter einmal negativ werden, gibt es keine zulässige Ressourcenzuteilung.

\subsubsection{Zulässigkeit bei hierarchischen Jobgruppen}
%Diese Situation kann mit einem ähnlichen Greedy-Algorithmus gelöst werden, wie bei disjunkten Ressourcenmengen.
%Die Ressourcen, die für eine Jobgruppe geeignet sind, können hier allerdings nicht als identisch angesehen werden,
%so dass es genügen würde, wieder nur die Anzahl der Ressourcen zu betrachten.
%Die Ressourcen unterscheiden sich dahingehend voneinander, dass sie für unterschiedlich viele andere Jobs ebenfalls geeignet sind.
%Der Greedy-Algorithmus muss so abgeändert werden, dass er jedem Job in $\pi$ immer die "`speziellste"' aktuell für diesen Job verfügbare Ressource zuweist.
%Soll einem Job $j$ eine Ressource zugeteilt werden, muss unter allen geeigneten Ressourcen $r\in\rho_j$, 
%wobei $r$ nicht schon einem anderen Job zugewiesen sein darf, diejenige mit minimalem $|\iota_r|$ ausgesucht werden.
%
%Dieser abgewandelte Greedy-Algorithmus ist korrekt, denn immer, wenn die Situation auftritt, dass für einen Job $j$ keine Ressource verfügbar ist,
%liegt das daran, dass alle potentiell geeigneten Ressourcen $\rho_j$ bereits an einen der $m-1$ direkten Vorgänger von $j$ vergeben worden sind.
%Sei $f$ das unvollständige Mapping, das der Algorithmus bis zu diesem Punkt aufgestellt hat.
%Sei $F\subseteq N$ die Menge der Vorgänger von $j$, denen eine Ressource $r\in\rho_j$ zugeteilt wurde.
%Angenommen, es existiert ein zulässiges Mapping $f'$, obwohl der Algorithmus $j$ keine Ressource zuweisen kann.
%Dann muss es einen Job $i\in F$ geben mit $f'(i)\not\in\rho_j$.
%Wegen der hierarchischen Eigenschaft muss entweder $\iota_{f'(i)}\subset\iota_{f(i)}$ sein oder $\iota_{f(i)}\subseteq\iota_{f'(i)}$.
%Da $f(i)\not\in\rho_j$ ist (und somit auch $j\not\in\iota_{f'(i)}$),
%muss hier $\iota_{f'(i)}\subset\iota_{f(i)}$ sein, denn sonst wäre $j\in\iota_{f(i)}\subseteq\iota_{f'(i)}$.
%Das wiederum bedeutet, dass auch der Algorithmus dem Job $i$ die speziellere Ressource $f'(i)$ zugewiesen hätte.
%Da er das aber nicht getan hat, muss er sie stattdessen einem der $m-1$ direkten Vorgänger von $i$ zugewiesen haben.
%Sei $h$ dieser Vorgänger von $i$.
%Es ist also $f(h)=f'(i)$ und somit $f'(h)\neq f'(i)$.


\subsubsection{Zulässigkeit bei beliebigen Ressourcenteilmengen}
\label{subsubsec:ZulaessigkeitBeiBeliebigenRessourcenteilmengen}
Es ist nicht bekannt, ob das Problem, zu entscheiden, ob es bei gegebenem $\pi$ und beliebigen Ressourcenmengen ein zulässiges Mapping $f$ gibt, 
$\mathcal{NP}$-vollständig ist oder ob es einen polynomiellen Algorithmus gibt.

Es besteht allerdings die Vermutung, dass es zumindest bei konstantem $m$ einen polynomiellen Algorithmus in $n$ gibt. Ein Indiz für diese Vermutung liefert folgendes MIP:
\begin{align}
    \text{max} \quad &0 \label{zulmip:obj} \\
    \text{s.t.}\quad \sum_{r\in\rho_i} x_{ir} &= 1 & i\in N \label{zulmip:1} \\
    x_{ir} + x_{jr} &\leq 1 & i\in N, j=i+1,\ldots,i+m-1 \, ,r\in\rho_i\cap\rho_j \label{zulmip:2} \\
    x_{ir} &\in \{0,1\} & i\in N, r\in\rho_i
\end{align}
Die Binärvariablen $x_{ir}$ geben hier an, ob dem Job $i$ die Ressource $r$ zugeteilt wird.
In diesem Fall ist $x_{ir}=1$ und sonst $x_{ir}=0$.
Die erste Nebenbedingung \ref{zulmip:1} bewirkt, dass jedem Job genau eine Ressource zugewiesen wird.
Durch die zweite Nebenbedingung \ref{zulmip:2} kann jede Ressource jeweils nur einmal an $m$ aufeinander folgende Jobs vergeben werden.
Eine Zielfunktion ist nicht notwendig, da die Zulässigkeit nur durch die Nebenbedingungen entschieden wird.
Mit diesem MIP können selbst sehr große Instanzen mit $n\approx 100000$ (allerdings mit $m\leq 8$) in weniger als einer Sekunde gelöst werden.

Nebenbedingung \ref{zulmip:2} verhindert immer nur für Paare von Jobs, deren Abstand kleiner als $m$ ist, dass ihnen die selbe Ressource zugewiesen wird.
Das ist für die Korrektheit des MIPs zwar ausreichend, durch zusätzliche Nebenbedingungen, die dasselbe für Tripel, Quadrupel, bis hin zu $m$-Tupeln verhindern,
können aber einige fraktionale Lösungen der Relaxation "`abgeschnitten"' werden.
Für ein $k$-Tupel von Jobs, die alle in einem Abschnitt von $\pi$ der Länge $m$ stehen, sieht diese zusätzliche Nebenbedingung so aus:
\begin{align}
    \sum_{l=1}^k x_{i_lr} \leq 1 \quad i_l\in N, i_1<i_2<\ldots<i_k<i_1+m, r\in\bigcap_{l=1}^k \rho_{i_l}
\end{align}
Für $k=2$ ist diese Nebenbedingung identisch mit \ref{zulmip:2}.
Wird sie für $k=3,\ldots,m$ zu obigem MIP hinzugefügt, so war bei allen bislang getesteten Instanzen die Relaxation bereits ganzzahlig.
Ein Beweis, dass dies tatsächlich bei allen Instanzen der Fall ist, ist noch nicht gelungen.
Gelingt er, ist bewiesen, dass es in polynomieller Zeit möglich ist, zu entscheiden, ob es für eine gegebene Jobreihenfolge $\pi$ und 
gegebene (beliebige) Ressourcenteilmengen $\rho_i$ möglich ist, allen Jobs eine Ressource zuzuweisen.


\subsection{Optimierung der Ressourcenzuweisung}
\label{subsec:OptimierungDerRessourcenzuweisung}
Da unbekannt ist, ob das Finden \textit{irgendeiner} Ressourcenzuweisung $\mathcal{NP}$-schwer ist,
ist es natürlich ebenfalls unbekannt, ob das Finden einer \textit{optimalen} Ressourcenzuweisung $\mathcal{NP}$-schwer ist.

Eine Ausnahme bildet der Fall von disjunkten Ressourcenteilmengen $\rho_i$.
Immer, wenn zwei Jobs aus einer Familie im Abstand $m$ aufeinander folgen, kann ihnen dieselbe Ressource zugewiesen werden.
Wenn zwei Jobs im Abstand $m$ nicht zur selben Familie gehören, gibt es auch keine Möglichkeit, Rüstkosten zu vermeiden oder zu verringern.

Für den allgemeinen Fall lässt sich das MIP aus Unterabschnitt \ref{subsubsec:ZulaessigkeitBeiBeliebigenRessourcenteilmengen} erweitern:
\begin{align}
    \text{max} \quad \sum_{i=m}^n\sum_{r\in\rho_i\cap\rho_{i-m}} &y_{ir} \label{optmip:obj} \\
    \text{s.t.}\quad \sum_{r\in\rho_i} x_{ir} &= 1 & i\in N \label{optmip:1} \\
    x_{ir} + x_{jr} &\leq 1 & i\in N, j=i+1,\ldots,i+m-1 \, ,r\in\rho_i\cap\rho_j \label{optmip:2} \\
    y_{ir} &\leq x_{ir} & i=m,\ldots,n \, ,r\in\rho_i\cap\rho_{i-m} \label{optmip:3}\\
    y_{ir} &\leq x_{i-m,r} & i=m,\ldots,n \, ,r\in\rho_i\cap\rho_{i-m} \label{optmip:4}\\
    x_{ir} &\in \{0,1\} & i\in N, r\in\rho_i \\
    y_{ir} &\in \{0,1\} & i=m,\ldots,n \, ,r\in\rho_i\cap\rho_{i-m}
\end{align}
Für die Binärvariablen $y_{ir}$ gilt $y_{ir}=1$ genau dann, wenn den Jobs an den Positionen $i$ und $i-m$ die selbe Ressource $r$ zugeteilt ist.
Diese Eigenschaft wird durch die Nebenbedingungen \ref{optmip:3} und \ref{optmip:4} erzwungen.
In der Zielfunktion \ref{optmip:obj} wird daher die Anzahl der $y$-Variablen, die auf $1$ gesetzt sind, maximiert, da so die wenigsten Rüstkosten auftreten.
Dieses MIP liefert genau wie jenes aus \ref{subsubsec:ZulaessigkeitBeiBeliebigenRessourcenteilmengen} in weniger als einer Sekunde auch für sehr große Instanzen eine Lösung.
Dieser Umstand lässt vermuten, dass auch das Optimierungsproblem der Ressourcenzuteilung in polynomieller Zeit lösbar ist.

\section{Unzulässige Reihenfolgen}
\label{UnzulaessigeReihenfolgen}
Für den Fall, dass für eine gegebene Reihenfolge $\pi$ kein zulässiges Mapping $f$ erstellt werden kann, muss $\pi$ nachträglich verändert werden.

\subsection{Nachbarschaftssuche}

\textit{Anfang August hiermit anfangen.}

Nachbarschaftssuchen formulieren, die eine unzulässige Reihenfolge reparieren, so dass sie dann zulässig ist.
Dabei soll die Reihenfolge möglichst wenig von ihrer Optimalität einbüßen.

\subsection{Andere Ansätze}
Platzhalter, falls noch andere Ideen aufkommen.






\chapter{Der zweite Dekompositionsansatz}
In diesem Kapitel wird der zweite Dekompositionsansatz vorgestellt.
Hierbei wird erst jedem Job eine Ressource zugewiesen, also ein Mapping $f$ erzeugt,
und anschließend werden die Jobs in eine Reihenfolge $\pi$ gebracht.
Neben der reinen Ressourcenzuteilung wird im ersten Schritt auch teilweise schon festgelegt, welche Jobs in der späteren Reihenfolge einen Abstand von $m$ haben sollen.
So wird sichergestellt, dass die durch das Mapping $f$ vorgeschriebenen Ressourcenzuteilungen auch dazu führen, dass Rüstkosten eingespart werden.

In Abschnitt \ref{sec:Ressourcenzuweisung} wird erklärt, wie ein optimales Mapping $f$ mit möglichst wenigen Rüstkosten mit einem Binpacking-Ansatz erstellt werden kann.
Im nächsten Abschnitt \ref{sec:ZulaessigkeitDerZuweisung} wird diskutiert, ob das in Abschnitt \ref{sec:Ressourcenzuweisung} beschriebene Verfahren
auch unzulässige Mappings erzeugen kann bzw. wie diese Situation verhindert oder behoben werden kann.
Anschließend wird in Abschnitt \ref{sec:AnordnungDerJobgruppen} beschrieben, wie basierend auf dem gegebenen Mapping $f$ eine Reihenfolge $\pi$ erstellt wird,
so dass die Summe der Zykluszeiten ($C_{\max}$) minimal ist.


\section{Ressourcenzuweisung}
\label{sec:Ressourcenzuweisung}
In diesem Abschnitt wird die Konstruktion des Mappings $f$ erläutert.
Je nachdem, ob die Rüstkosten von beiden, Vorgänger- und Nachfolgerjobgruppe, nur vom Nachfolger oder von keinem von beiden abhängt, also konstant ist,
sind unterschiedliche Verfahren bekannt.
Im ersten Unterabschnitt \ref{subsec:BekannteLaufzeitschrankenUndVerfahren} werden zunächst bekannte Laufzeitschranken und die zugehörigen Verfahren 
für die einzelnen Fälle kurz aufgezählt.
Anschließend wird in Unterabschnitt \ref{subsec:RessourcenzuweisungMitBinpacking} auf den Spezialfall $s_{fg}=s$, der mit einem Binpackingansatz gelöst werden kann,
eingegangen.

In realen Anwendungsfällen ist es in häufig nicht der Fall, dass bei $n$ zu produzierenden Jobs alle $n$ Jobs unterschiedlich sind,
d.h. unterschiedliche Prozesszeiten und Ressourcen haben.
Stattdessen bietet ein Produzent eine gewisse Anzahl unterschiedlicher Güter an und ein Käufer gibt mehrere identische Güter auf einmal in Auftrag.
Im Folgenden ist daher von \textit{Jobgruppen} die Rede, wobei eine Jobgruppe $g$ aus $n_g\in\mathbb{N}$ identischen Jobs besteht.
$n_g$ istalso die Größe einer Jobgruppe $g$.
Die Anzahl der Jobgruppen wird mit $\tilde{n}$ bezeichnet.
Für dei Gesamtanzahl an Jobs gilt dann $n = \sum_{g=1}^{\tilde{n}} n_g$.

\subsection{Bekannte Laufzeitschranken und Verfahren}
\label{subsec:BekannteLaufzeitschrankenUndVerfahren}

\subsection{Ressourcenzuweisung mit Binpacking}
\label{subsec:RessourcenzuweisungMitBinpacking}
Bei $\tilde{n}$ gegebenen Jobgruppen, wobei alle Jobs aus einer Gruppe die gleichen Ressourcen verwenden können,
ist es naheliegend, die Jobs einer Gruppe immer im Abstand von $m$ in die Anlage einzulegen.
Auf diese Weise muss die Ressource in der entsprechenden Station so lange nicht ausgetauscht werden,
bis alle Jobs aus einer Jobgruppe fertiggestellt worden sind.
Erst danach, wenn Jobs aus einer anderen Gruppe eingelegt werden, muss einmalig die Ressource gewechselt werden,
so dass nur dann Rüstkosten auftreten.
Dabei soll zunächst nur der Spezialfall mit disjunkten Ressourcenmengen und konstanten Rüstkosten ($s_{fg}=s$) betrachtet werden.

O.B.d.A. sei $n$ ein Vielfaches von $m$, denn andernfalls kann eine weitere "`Dummy"'-Jobgruppe $g_d$ konstruiert werden,
deren Jobs die Prozesszeiten $0$ haben und die genau so groß ist, dass $n+n_{g_d}$ ein Vielfaches von $m$ ist.
Bei $\tilde{n}$ gegebenen Jobgruppen mit den Größen $n_g, \, g=1,\ldots,\tilde{n}$ ist das Ziel,
die Jobgruppen in $m$ gleichgroße Teilmengen zu unterteilen.
Dieses Problem lässt sich als Binpacking-Instanz betrachten, wobei entschieden werden muss, ob $\tilde{n}$ Items 
mit den Größen $n_1,\ldots,n_{\tilde{n}}$ in $m$ Bins der Größe $\frac{n}{m}$ untergebracht werden können.
Für den Fall, dass es möglich ist, müssen noch die Bins den Stationen der Anlage zugeordnet werden und die Reihenfolge
der Jobgruppen in jedem Bin muss so bestimmt werden, dass die Zykluszeiten minimiert werden.
Dies wird in Abschnitt \ref{sec:AnordnungDerJobgruppen} beschrieben.
Falls dies nicht möglich ist, bedeutet das, dass mindestens eine der Jobgruppen aufgeteilt werden muss auf zwei Bins,
so dass ein weiteres Mal Rüstkosten auftreten.
Um herauszufinden, welche Jobgruppe geteilt werden muss und wie groß die beiden Hälften sein müssen,
wird eine weitere Binpacking-Instanz gelöst, die bis auf eine Änderung identisch zur vorherigen ist:
Es gibt nicht mehr $m$ Bins, sondern nur noch $m-1$ und einer dieser Bins hat die Größe $2\frac{n}{m}$ (alle anderen haben weiterhin die Größe $\frac{n}{m}$).
Wenn hier eine Aufteilung möglich ist, kann der Bin mit doppelter Größe wieder halbiert werden,
wobei dann eine der Jobgruppen in ihm auch geteilt werden muss wie in Abbildung \ref{abb:binhalb} veranschaulicht.
Sollte auch hier keine Lösung möglich sein, muss eine weitere Jobgruppe geteilt werden.
Dazu wird wiederum eine neue Binpacking-Instanz erzeugt.
Abbildung \ref{abb:binpart} zeigt, wie die Bingrößen in den weiteren Instanzen aussehen.
Es handelt sich dabei um die \textit{ungeordnete Zahlenpartition} von $m$.
Spätestens bei der Binpacking-Instanz, die nur noch einen Bin enthält mit der Größe $n$, gibt es eine Lösung,
bei der dann $m$ Jobgruppen aufgeteilt werden müssen, so dass $m$ weitere Ressourcenwechsel und damit Rüstkosten auftreten.

Das Binpacking-Problem ist zwar $\mathcal{NP}$-vollständig, da aber sowohl die Anzahl der Bins $m$ 
als auch die Anzahl der Items bzw. Jobgruppen $\tilde{n}$ meist relativ klein sind (meist deutlich kleiner als die Anzahl Jobs $n$),
lässt sich das Problem, die Jobgruppen den Bins zuzuordnen und ggf. geeignete Jobgruppen zu finden, die aufgeteilt werden müssen, 
trotzdem in hinnehmbarer Zeit mit folgendem MIP lösen: 
\begin{align}
    \text{max} \quad &0 \\
    \text{s.t.}\quad \sum_{j=1}^{m'} x_{gj} &= 1 &g=1,\ldots,\tilde{n}\\
    \sum_{g=1}^{\tilde{n}} n_g x_{gj} &= B_j &j=1,\ldots,m' \\
    x_{gj} &\in \{0,1\} &g=1,\ldots,\tilde{n}, \, j=1,\ldots,m'
\end{align}
Für die Binärvariablen $x_{gj}$ gilt: $x_{gj}=1$ genau dann, wenn Jobgruppe $g$ in Bin $j$ ist.
Die erste Nebenbedingung fordert, dass jede Jobgruppe in genau einem Bin ist 
und die Zweite Nebenbedingung verlangt, dass jeder Bin exakt gefüllt ist.
Dabei sind $B_j$ die Größen der Bins, die zusammen mit der Anzahl der Bins $m'$ mit jedem Aufruf einer Zahlenpartition von $m$ entsprechen.
In Abbildung \ref{abb:binzeit} sind die Laufzeiten dieses MIPs aufgetragen.
Dabei wurde das MIP in ZIMPL formuliert und mit CPLEX gelöst.
Die Partitionen von $m$ wurden der Reihenfolge nach (vgl. Abbildung \ref{binpart}) mit kleiner werdendem $m'$ durchlaufen,
bis eine zulässige Aufteilung gefunden wurde.


\subsection{Zulässigkeit der Zuweisung}
\label{subsec:ZulaessigkeitDerZuweisung}
In Unterabschnitt \ref{subsec:RessourcenzuweisungMitBinpacking} wurde erläutert, wie Jobs bzw. Jobgruppen so angeordnet werden können,
dass möglichst selten eine Ressource gewechselt werden muss, was Rüstkosten verursacht.
Es wurde allerdings davon ausgegangen, dass immer genug Ressourcen vorhanden sind,
d.h. es wurde nur optimiert, ohne zu beachten, ob diese Zuweisung von Ressourcen überhaupt zulässig ist.
Die drei Fälle, wie die Ressourcen verteilt sein können (disjunkt, hierarchisch, beliebig) sollen hier deshalb diskutiert werden.

\subsubsection{Zulässigkeit bei disjunkten Ressourcenmengen}
O.B.d.A. kann davon ausgegangen werden, dass es für jede Jobgruppe mindestens eine zulässige Ressource gibt.
Andernfalls könnten diese Jobs niemals produziert werden.
Außerdem kann mit dem oben beschriebenen Verfahren eine Jobgruppe $g$ nur dann mehr als einmal geteilt werden, wenn $n_g > \frac{n}{m}$ ist.
Bei einer $k$-fachen Teilung von $g$ gilt für die resultierenden Teilgruppen $n_{g_1},n_{g_k}\leq\frac{n}{m}$ und $n_{g_i}=\frac{n}{m}$ für $i=2,\ldots,k$.
Allen Teilgruppen mit $g_i=\frac{n}{m}$ kann eine Ressource fest zugeteilt werden, da sich Jobs dieser Teilgruppen durchgehend auf einer Station befinden.
Sie können daher als eigenständige Jobgruppen mit genau einer zulässigen Ressource aufgefasst werden.
Für $g_1$ und $g_k$ stehen dann entsprechend weniger Ressourcen zur Verfügung.
Daher kann o.B.d.A. davon ausgegangen werden, dass jede Jobgruppe höchstens einmal aufgeteilt werden kann.
Eine Jobgruppe, die nicht geteilt wird, benötigt nur eine Ressource.
Es muss also nur der Fall betrachtet werden, dass es für eine Jobgruppe $g$, die geteilt wird, nur eine Ressource gibt.
Denn dann dürfen sich zu keinem Zeitpunkt Jobs aus den beiden Teilgruppen $g_1$ und $g_2$ gleichzeitig in der Anlage befinden.

Falls $n_g \leq \frac{n}{m}$ ist, ist es immer möglich, die beiden Teilgruppen, die aus $g$ hervorgehen,
so in den Bins anzuordnen, dass niemals zwei Jobs aus $g$ zur gleichen Zeit in der Anlage sind, indem beispielsweise
$g_1$ an den Anfang ihres Bins gelegt wird und $g_2$ ans Ende von ihrem Bin (vgl. Abbildung \ref{abb:blabla}).
Diese Bedingung, dass $g_1$ und $g_2$ sich nicht überlappen dürfen, muss allerdings in den nachfolgenden Verfahren, 
die $C_{\max}$ minimieren (s. Abschnitt \ref{sec:AnordnungDerJobgruppen}), beachtet werden.
Falls mehrere Jobgruppen mit $n_g\leq\frac{n}{m}$ geteilt werden (es können höchstens $m$ sein), können in jedem Bin höchstens zwei resultierende 
Teilgruppen vorkommen, so dass es immer möglich ist, eine von beiden an den Anfang und die andere an das Ende das Bins zu legen (vgl. Abbildung \ref{abb:bla}).

Falls eine Jobgruppe $g$ mit $n_g > \frac{n}{m}$ geteilt wird,
gibt es keine Möglichkeit, $g$ so aufzuteilen, 
dass sich in jedem Zyklus höchstens ein Job aus $g$ in der Anlage befindet.
da nur $\frac{n}{m}$ Zyklen gemacht werden. 
Da $g$ zu groß ist für einen Bin, muss $g$ aber zwangsweise geteilt werden, so dass bei nur einer geeigneten Ressource definitiv keine zulässige Lösung existiert
(dieser Umstand kann bereits im Voraus geprüft werden, so dass dieser Fall hier o.B.d.A. nicht auftritt).
Von dieser Regel gibt es allerdings eine Ausnahme:
Falls $n_g = \frac{n}{m}+1$ ist, kann $g_1$ in ihrem Bin an vorderster Stelle eingeplant werden und $g_2$ in ihrem Bin an hinterster Stelle.
Wenn dann der Bin von $g_1$ Station $1$ und der von $g_2$ Station $m$ zugeordnet wird, ist trotzdem in jedem Zyklus nur ein Job aus $g$ in der Anlage 
wie in Abbildung \ref{abb:blabalbal} dargestellt.
Von dieser Ausnahme kann allerdings höchstens einmal Gebrauch gemacht werden.

Das Probleme, dass eine Jobgruppe mit nur einer Ressource geteilt wird, kann vermieden werden, indem beim Binpacking, 
sobald die Notwendigkeit der Teilung einer Jobgruppe festgestellt wurde,
nicht eine beliebige Jobgruppe ausgewählt wird, sondern eine, für die es mindestens zwei Ressourcen gibt.
Wenn solch eine Jobgruppe nicht zur Auswahl steht, kann trotzdem eine mit $n_g\leq\frac{n}{m}$ ausgewählt werden, was dann zusätzliche Bedingungen für die
nachfolgenden Verfahren verursacht. Wenn auch solch eine Jobgruppe nicht existiert, muss eine neue Binpacking-Instanz mit der nächsten Zahlenpartition von $m$ gelöst werden.
Erst, wenn alle Partitionen getestet wurden, steht fest, dass keine zulässige Ressourcenzuteilung existiert.

\subsubsection{Zulässigkeit bei hierarchischen Ressourcenmengen}


\subsubsection{Zulässigkeit bei beliebigen Ressourcenmengen}


\section{Anordnung der Jobgruppen}
\label{sec:AnordnungDerJobgruppen}
In diesem Abschnitt werden Verfahren zur Anordnung der Jobgruppen in den Bins einerseits und
zum Anordnen der Bins untereinander andererseits vorgestellt.
Aus den vorangegangenen Verfahren (vgl. Abschnitt \ref{sec:Ressourcenzuweisung}) ist bekannt, zu welchen Bins die Jobgruppen gehören.
Geteilte Jobgruppen werden hier als eingenständige Gruppen betrachtet.
Auch eventuelle zusätzliche Restriktionen, dass die Teile einer geteilten Jobgruppe sich nicht überschneiden dürfen, sind bekannt.
Ziel ist es, $C_{\max}$ zu minimieren, ohne die Jobgruppen zu teilen und somit die bereits minimierten Rüstkosten zu verändern.


\subsection{MIP mit fixierten Bins}
\label{subsec:MIPMitFixiertenBins}
Hier wird zunächst ein MIP vorgestellt, dass lediglich die Jobgruppen in den Bins anordnet.
Die Bins sind dabei bereits fest den Stationen zugeordnet (der erste Bin zur ersten Station usw.).
Dieses MIP ist allerdings nur anwendbar bei zwei dominierenden Maschinen, die nicht notwendiger Weise benachbart sein müssen.
Ein MIP, das auch die Bins untereinander anordnet, d.h. den Stationen zuordnet, wird in Unterabschnitt \ref{MIPMitFreienBins} vorgestellt.

Mit $\tilde{N}=\{1,\ldots,\tilde{n}\}$ wird die Menge der Jobgruppen bezeichnet.
$B=\{1,\ldots,m\}$ ist die Menge der Bins und für $b\in B$ ist $\tilde{N_b}\subset \tilde{N}$ die Teilmenge der Jobgruppen in Bin $b$.
Für $i\in\tilde{N}$ gibt $b_i\in B$ den Bin an, in dem sich $i$ befindet.
$\tilde{N_b}^*\in \tilde{N}$ bezeichnet die Menge der Jobgruppen im `"Nachbarbin'" von $b$.
Der Nachbarbin von $b$ ist derjenige, dessen Jobs auf der ersten dominierenden Maschine sind, während die Jobs von $b$ gerade auf der zweiten dominierenden Maschine sind.
Wenn die beiden dominierenden Maschinen beispielsweise einen Abstand von $2$ haben, dann ist der Nachbarbin eines Bins auch um zwei Bins weiter rechts (modulo $m$).
$p_{1i}$ und $p_{2i}$ sind die Prozesszeiten auf den beiden dominierenden Maschinen.
Der Parameter $\sigma_{ij}$ hat für je zwei Jobgruppen $i\in\tilde{N},j\in\tilde{N}^*_{b_i}$ den Wert $1$, wenn $b_j<b_i$, und $0$ sonst.
$\sigma_{ij}$ gibt also an, ob bei der Berechnung der Überlappung der Jobgruppen $i$ und $j$ eine Verschiebung um $1$ stattfinden muss (vgl. Abbildung \ref{abb:bla}).
\begin{align}
    \text{min}\quad &\sum_{b\in B}\sum_{\substack{i\in \tilde{N_b}\\j\in \tilde{N}_b^*}} 
            \max\{p_{2i},p_{1j}\}o_{ij} \label{objfunc}\\
			\text{s.t.} \quad x_{ij} + x_{ji} &= 1 &i\in \tilde{N}, j\in \tilde{N}_{b_i} 
            \label{con:reflex}\\
			x_{ij} + x_{jk} &\leq x_{ik} + 1 &i\in \tilde{N},\,\, j,k\in \tilde{N}_{b_i}
            \label{con:transitiv} \\
			s_i &= \sum_{\substack{j\in \tilde{N}_{b_i}\\i\neq j}} g_j x_{ji} &i\in \tilde{N} 
            \label{con:start} \\
			My_{ij} &\geq s_j - s_i + \sigma_{ij}+ \epsilon &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    M(1-y_{ij}) &\geq s_i - s_j - \sigma_{ij} &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    Mz_{ij} &\geq s_i + g_i - s_j - g_j - \sigma_{ij} + \epsilon &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    M(1-z_{ij}) &\geq s_j + g_j- s_i - g_i + \sigma_{ij} + \epsilon &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    o_{ij} &\geq s_i + g_i - s_j - \sigma_{ij} - M(1-y_{ij}+z_{ij}) &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    o_{ij} &\geq s_j + g_j - s_i + \sigma_{ij} - M(1-y_{ji}+z_{ji}) &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    o_{ij} &\geq g_j - M(2-y_{ij}-z_{ij}) &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} \\
		    o_{ij} &\geq g_i - M(2-y_{ji}-z_{ji}) &i\in \tilde{N}, j\in \tilde{N}^*_{b_i} 
\end{align}

Außerdem die erweiterte Formulierung auf allgemeine $s_{fg}$ und Diskussion.
\textit{MIP sollte bis Ende Juli formuliert und Laufzeiten gemessen sein.}

\subsection{MIP mit freien Bins}
\label{subsec:MIPMitFreienBins}
\textit{Muss nur noch aufgeschrieben werden. Evtl. noch ein paar Laufzeiten messen.}

Vorstellung des MIPs mit freien Bins. 
Die Laufzeit ist sehr viel schlechter, die primale Lösung ist aber meist schon nach kurzer Zeit besser als beim MIP mit fixierten Bins.

\subsection{Mehrere fixierte Reihenfolgen}
\textit{Jetzt damit anfangen. Wahrscheinlich bis Ende August.}

Basierend auf den Erkenntnissen aus \ref{subsec:mipfreibins} das MIP mit fixierten Bins für mehrere Binreihenfolgen laufen lassen.
Um nicht sämtliche Binreihenfolgen durchprobieren zu müssen,
ggf. mit bipartitem gewichteten Matching ein lokales Optimum zwischen zwei Bins suchen und daraus eine "`gute"' Binreihenfolge erstellen.


\chapter{Rechenergebnisse und Vergleiche}
\textit{Anfang September anfangen, soweit Messergebnisse vorliegen.}

Obwohl in den vorherigen Kapiteln schon einige Laufzeiten vorgestellt werden, hier nochmal eine Zusammenfassung und insbesondere ein Vergleich
zwischen den beiden Dekompositionsansätzen. Sowohl echte Instanzen als auch generierte. 
Dabei ggf. auf Methoden zur Instanzengenerierung eingehen und diese bewerten?



\newpage
\bibliographystyle{alphadin}
\bibliography{quellen}


\end{document}
